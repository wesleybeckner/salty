{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import salty\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can check the output of this cell to kind of see how much new structural data we acquire with more properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wab665/anaconda3/envs/py36/lib/python3.6/site-packages/pandas/core/generic.py:4384: FutureWarning: Attribute 'is_copy' is deprecated and will be removed in a future version.\n",
      "  object.__getattribute__(self, name)\n",
      "/Users/wab665/anaconda3/envs/py36/lib/python3.6/site-packages/pandas/core/generic.py:4385: FutureWarning: Attribute 'is_copy' is deprecated and will be removed in a future version.\n",
      "  return object.__setattr__(self, name, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(503,)\n",
      "(534,)\n",
      "(628,)\n",
      "(628,)\n",
      "(688,)\n"
     ]
    }
   ],
   "source": [
    "properties = ['density', 'cpt', 'viscosity', 'thermal_conductivity',\n",
    "              'melting_point']\n",
    "for i in range(len(properties)):\n",
    "    props = properties[:i+1]\n",
    "    devmodel = salty.aggregate_data(props, merge='Union')\n",
    "    devmodel.Data['smiles_string'] = devmodel.Data['smiles-cation'] + \".\" + devmodel.Data['smiles-anion']\n",
    "    values = devmodel.Data['smiles_string'].drop_duplicates()\n",
    "    print(values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wab665/anaconda3/envs/py36/lib/python3.6/site-packages/pandas/core/generic.py:4384: FutureWarning: Attribute 'is_copy' is deprecated and will be removed in a future version.\n",
      "  object.__getattribute__(self, name)\n",
      "/Users/wab665/anaconda3/envs/py36/lib/python3.6/site-packages/pandas/core/generic.py:4385: FutureWarning: Attribute 'is_copy' is deprecated and will be removed in a future version.\n",
      "  return object.__setattr__(self, name, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(688,)\n"
     ]
    }
   ],
   "source": [
    "properties = ['density', 'cpt', 'viscosity', 'thermal_conductivity',\n",
    "              'melting_point']\n",
    "props = properties\n",
    "devmodel = salty.aggregate_data(props, merge='Union')\n",
    "devmodel.Data['smiles_string'] = devmodel.Data['smiles-cation'] + \".\" + devmodel.Data['smiles-anion']\n",
    "values = devmodel.Data['smiles_string'].drop_duplicates()\n",
    "print(values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    }
   ],
   "source": [
    "smile_max_length = values.map(len).max()\n",
    "print(smile_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_smiles(smiles_string, smile_max_length):\n",
    "     if len(smiles_string) < smile_max_length:\n",
    "            return smiles_string + \" \" * (smile_max_length - len(smiles_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_smiles =  [pad_smiles(i, smile_max_length) for i in values if pad_smiles(i, smile_max_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(padded_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_char_list(char_set, smile_series):\n",
    "    for smile in smile_series:\n",
    "        char_set.update(set(smile))\n",
    "    return char_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_set = set()\n",
    "char_set = create_char_list(char_set, padded_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " '#',\n",
       " '(',\n",
       " ')',\n",
       " '+',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '=',\n",
       " '@',\n",
       " 'B',\n",
       " 'C',\n",
       " 'F',\n",
       " 'H',\n",
       " 'I',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'S',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " 'c',\n",
       " 'e',\n",
       " 'l',\n",
       " 'n',\n",
       " 'r'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(char_set))\n",
    "char_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_list = list(char_set)\n",
    "chars_in_dict = len(char_list)\n",
    "char_to_index = dict((c, i) for i, c in enumerate(char_list))\n",
    "index_to_char = dict((i, c) for i, c in enumerate(char_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(': 0,\n",
       " '3': 1,\n",
       " '=': 2,\n",
       " 'l': 3,\n",
       " 'O': 4,\n",
       " 'n': 5,\n",
       " '1': 6,\n",
       " '-': 7,\n",
       " '#': 8,\n",
       " '+': 9,\n",
       " 'C': 10,\n",
       " ')': 11,\n",
       " 'r': 12,\n",
       " 'S': 13,\n",
       " '\\\\': 14,\n",
       " '2': 15,\n",
       " 'F': 16,\n",
       " '.': 17,\n",
       " 'e': 18,\n",
       " 'P': 19,\n",
       " 'c': 20,\n",
       " 'H': 21,\n",
       " ']': 22,\n",
       " ' ': 23,\n",
       " '[': 24,\n",
       " 'N': 25,\n",
       " 'I': 26,\n",
       " '/': 27,\n",
       " 'B': 28,\n",
       " '@': 29}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((len(padded_smiles), smile_max_length, chars_in_dict), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(687, 105, 30)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, smile in enumerate(padded_smiles):\n",
    "    for j, char in enumerate(smile):\n",
    "        X_train[i, j, char_to_index[char]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(X_train, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460, 105, 30)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to build RNN to encode. some issues include what the 'embedded dimension' is (vector length of embedded sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so some keras version stuff. 1.0 uses keras.losses to store its loss functions. 2.0 uses objectives. we'll just have to be consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.objectives import binary_crossentropy #objs or losses\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.layers.core import Dense, Activation, Flatten, RepeatVector\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.convolutional import Convolution1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I've adapted the exact architecture used in the [paper](https://github.com/maxhodak/keras-molecules/blob/master/molecules/model.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encoder(x, latent_rep_size, smile_max_length, epsilon_std = 0.01):\n",
    "    h = Convolution1D(9, 9, activation = 'relu', name='conv_1')(x)\n",
    "    h = Convolution1D(9, 9, activation = 'relu', name='conv_2')(h)\n",
    "    h = Convolution1D(10, 11, activation = 'relu', name='conv_3')(h)\n",
    "    h = Flatten(name = 'flatten_1')(h)\n",
    "    h = Dense(435, activation = 'relu', name = 'dense_1')(h)\n",
    "\n",
    "    def sampling(args):\n",
    "        z_mean_, z_log_var_ = args\n",
    "        batch_size = K.shape(z_mean_)[0]\n",
    "        epsilon = K.random_normal(shape=(batch_size, latent_rep_size),\n",
    "                                  mean=0., stddev = epsilon_std)\n",
    "        return z_mean_ + K.exp(z_log_var_ / 2) * epsilon\n",
    "\n",
    "    z_mean = Dense(latent_rep_size, name='z_mean', activation = 'linear')(h)\n",
    "    z_log_var = Dense(latent_rep_size, name='z_log_var', activation = 'linear')(h)\n",
    "\n",
    "    def vae_loss(x, x_decoded_mean):\n",
    "        x = K.flatten(x)\n",
    "        x_decoded_mean = K.flatten(x_decoded_mean)\n",
    "        xent_loss = smile_max_length * binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - \\\n",
    "                                 K.exp(z_log_var), axis = -1)\n",
    "        return xent_loss + kl_loss\n",
    "\n",
    "    return (vae_loss, Lambda(sampling, output_shape=(latent_rep_size,),\n",
    "                             name='lambda')([z_mean, z_log_var]))\n",
    "\n",
    "def Decoder(z, latent_rep_size, smile_max_length, charset_length):\n",
    "    h = Dense(latent_rep_size, name='latent_input', activation = 'relu')(z)\n",
    "    h = RepeatVector(smile_max_length, name='repeat_vector')(h)\n",
    "    h = GRU(501, return_sequences = True, name='gru_1')(h)\n",
    "    h = GRU(501, return_sequences = True, name='gru_2')(h)\n",
    "    h = GRU(501, return_sequences = True, name='gru_3')(h)\n",
    "    return TimeDistributed(Dense(charset_length, activation='softmax'),\n",
    "                           name='decoded_mean')(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape=(smile_max_length, len(char_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, z = Encoder(x, latent_rep_size=292, smile_max_length=smile_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(x, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoded_input looks like a dummy layer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = Input(shape=(292,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Model(encoded_input, Decoder(encoded_input, latent_rep_size=292,\n",
    "                                       smile_max_length=smile_max_length,\n",
    "                 charset_length=len(char_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a separate autoencoder model that combines the encoder and decoder (I guess the former cells are for accessing those separate parts of the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Input(shape=(smile_max_length, len(char_set)), name='input_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_loss, z1 = Encoder(x1, latent_rep_size=292, smile_max_length=smile_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Model(x1, Decoder(z1, latent_rep_size=292,\n",
    "                                       smile_max_length=smile_max_length,\n",
    "                 charset_length=len(char_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we compile and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='Adam', loss=vae_loss, metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 460 samples, validate on 227 samples\n",
      "Epoch 1/1\n",
      "460/460 [==============================] - 58s 126ms/step - loss: 11.6473 - acc: 0.5784 - val_loss: 8.4132 - val_acc: 0.5956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x116126438>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(X_train, X_train, shuffle = True, validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(a, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    a = np.log(a) / temperature\n",
    "    a = np.exp(a) / np.sum(np.exp(a))\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_smi = values[0]\n",
    "test_smi = pad_smiles(test_smi, smile_max_length)\n",
    "Z = np.zeros((1, smile_max_length, len(char_list)), dtype=np.bool)\n",
    "for t, char in enumerate(test_smi):\n",
    "    Z[0, t, char_to_index[char]] = 1\n",
    "    \n",
    "# autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " callback guess: I\\=C - + c ]  c(       ((n C)r      ( C)=   +)    SPn( C]     FS1FC      ( 3C((        )  ( )  C)  [)  ) \n"
     ]
    }
   ],
   "source": [
    "string = \"\"\n",
    "for i in autoencoder.predict(Z):\n",
    "    for j in i:\n",
    "        index = sample(j)\n",
    "        string += index_to_char[index]\n",
    "print(\"\\n callback guess: \" + string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CCCC[n+]1ccc(cc1)C.[B-](F)(F)(F)F'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460, 105, 30)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
